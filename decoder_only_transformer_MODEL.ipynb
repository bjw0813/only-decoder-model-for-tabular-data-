{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ft-DECODER MODEL을 바탕으로 만들었습니다.**"
      ],
      "metadata": {
        "id": "DSHBOf-fuZF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ6xdKeSuih4",
        "outputId": "697965da-2eca-45ba-f33c-2abab2fd8043"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m930.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from  sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from sklearn.preprocessing import LabelEncoder #라벨 인코더"
      ],
      "metadata": {
        "id": "64KpDBqfuZqj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nIm_Eguuhnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding 모듈"
      ],
      "metadata": {
        "id": "2z4aILEyurpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#연속형 변수 임베딩 함수\n",
        "def embedfornum(df):\n",
        "  linear=nn.Linear(4,20)\n",
        "  L=[] #행단위로 리스트화를 진행했다\n",
        "  for i in range(len(df.index)):\n",
        "    x=df.iloc[i].tolist()\n",
        "    z=pd.DataFrame(x)\n",
        "    L.append(x)\n",
        "  input=torch.Tensor(L)#리스트 텐서로 변환#이건 tensor로 전환후 하는 경우\n",
        "  x=linear(input)\n",
        "  return x"
      ],
      "metadata": {
        "id": "msTMDlpKuwDl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩텐서들 합치기\n",
        "def AddEmbedding(x,y,z):\n",
        "  proto_embedding=torch.cat((x,y), dim=1)\n",
        "  final_embedding=torch.cat((proto_embedding,z), dim=1)\n",
        "  final_embedding=final_embedding.tolist()\n",
        "  final_embedding=np.array(final_embedding)\n",
        "  fe=[]\n",
        "  for i in range(len(x)):\n",
        "    v=final_embedding[i].reshape(6,5).tolist()\n",
        "    fe.append(v)\n",
        "  final_embedding=torch.tensor(fe)\n",
        "  return final_embedding"
      ],
      "metadata": {
        "id": "H-70MIKeuyvt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super(Embedding, self).__init__()\n",
        "\n",
        "    self.embedding=nn.Embedding(num_embeddings=3,embedding_dim=5)\n",
        "    self.embedding1=nn.Embedding(num_embeddings=4,embedding_dim=5)\n",
        "    self.label_encoders = {}\n",
        "    self.label_encoders[\"type\"] = LabelEncoder()\n",
        "    self.label_encoders2 = {}\n",
        "    self.label_encoders2[\"lane\"] = LabelEncoder()\n",
        "    #self.embedding2=embedfornum(df_num)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=x.fillna(0)\n",
        "    df_num=x[[\"speed\",\"capacity\",\"cnt\",\"od\"]]\n",
        "    df_cat=x[[\"lane\",\"type\"]]\n",
        "\n",
        "\n",
        "    df_cat.loc[:, \"type\"] = self.label_encoders[\"type\"].fit_transform(df_cat[\"type\"])\n",
        "\n",
        "    df_cat.loc[:, \"lane\"] = self.label_encoders2[\"lane\"].fit_transform(df_cat[\"lane\"])\n",
        "    input=torch.LongTensor(df_cat[\"type\"].values)\n",
        "    embedding_type=self.embedding(input)\n",
        "    input1=torch.LongTensor(df_cat[\"lane\"].values)\n",
        "    embedding_lane=self.embedding1(input1)\n",
        "    embedding_num=embedfornum(df_num)\n",
        "    embedding_final=AddEmbedding(embedding_num,embedding_type,embedding_lane)\n",
        "\n",
        "    return embedding_final\n"
      ],
      "metadata": {
        "id": "5hMD2Ycuu0A5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머(디코더), 어텐션 포함(기존ft-transformer 모델 참고)"
      ],
      "metadata": {
        "id": "s-r-A5N0u9Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#attention으로 나온 정보들을 합쳐주는 역할\n",
        "def FeedForward(dim, mult = 4, dropout = 0.): #차원개수, 드롭아웃# mult는 내부게산을 위한 벡터 길이 설정값이다.\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, dim * mult * 2),\n",
        "        GEGLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(dim * mult, dim)\n",
        "    )"
      ],
      "metadata": {
        "id": "-0mapj7KvHsH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)#x를 2개로 분리\n",
        "        return x * F.gelu(gates) # 그 다음 gate값은 활성화 함수 씌워주고 두 벡터끼리 다시 곱해준다."
      ],
      "metadata": {
        "id": "X751UNT4vKtD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#multihead attention(self attention)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,#차원개수 attention하고 싶은 차원개수\n",
        "        heads = 8, #하이퍼 파라미터(본인이 정하는 것)(default 8임)\n",
        "        dim_head = 64,#벡터들의 차원(default:64였음)\n",
        "        dropout = 0 #드롭아웃비율(default=0)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads #초기 벡터들의 차원(512)\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5 #어텐션 스코어를 만드는것\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)#정규화\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)#qkv벡터를 얻는 레이어(가중치 행렬) #forward에서 chunk로 따로 분리해주게 된다\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = False)#최종 아웃풋의 shape을 잡아주는 역할\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)#드롭아웃(정확도를 높이는 효과)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.heads #\n",
        "\n",
        "        x = self.norm(x) #벡터를 정규화\n",
        "\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)#이 함수때문에 위에서 x3을 해준것, chunk로 분리해줌\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))#분리한 qkv에 lamda 함수를 맵핑하고 rearrange를 통해 head개수만큼 나눈다\n",
        "\n",
        "        q = q * self.scale#스케일링#어텐션 스코어\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k)# q벡터랑 k벡터 곱하는거#텐서연산 4차원텐서의 곱\n",
        "\n",
        "        attn = sim.softmax(dim = -1)#소프트맥스 함수, attention distribution\n",
        "        dropped_attn = self.dropout(attn)#정확도 높이고\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', dropped_attn, v)# attention 분포 * values #텐서연산 4차원텐서의 곱\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)#괄호의 의미는 h랑 d를 곱한다는 의미\n",
        "        out = self.to_out(out)#attention의 결과물\n",
        "\n",
        "        return out, attn"
      ],
      "metadata": {
        "id": "_p38-eFvvK6v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(  #모듈생성\n",
        "        self,\n",
        "        dim,#차원수\n",
        "        depth,#layer개수\n",
        "        heads,#헤드개수(병렬)\n",
        "        dim_head,#q,k,v벡터길이(차원)를 정하는 파라미터\n",
        "        attn_dropout,#드롭아웃비율\n",
        "        ff_dropout#드롭아웃비율\n",
        "    ):\n",
        "        super().__init__() #nn.module method 다 불러오기\n",
        "        self.layers = nn.ModuleList([]) # 모듈리스트 만들기\n",
        "\n",
        "        for _ in range(depth):#layer개수만큼 attention과 feefforward를 실행해서 모듈리스트에 저장(디코더모듈 만들기)\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout),\n",
        "                FeedForward(dim, dropout = ff_dropout),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, return_attn = False): #모듈값 계산\n",
        "        post_softmax_attns = []\n",
        "\n",
        "        for attn, ff in self.layers:#attention과 feedforward\n",
        "            attn_out, post_softmax_attn = attn(x) #attention(x)의 return 값인 out과 attn값을 각각 할당\n",
        "            post_softmax_attns.append(post_softmax_attn)#attn값은 리스트에 저장\n",
        "\n",
        "            x = attn_out + x #return 값인 out을 계속 x에 더해 나간다.\n",
        "            x = ff(x) + x #그리고 FeedForward의 return값도 더해 나감. x는 리스트형태의 input\n",
        "\n",
        "        if not return_attn:#return_attn이 False이므로 x를 리턴한다.\n",
        "            return x\n",
        "\n",
        "        return x, torch.stack(post_softmax_attns)"
      ],
      "metadata": {
        "id": "Sc8dKoCHvSeV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **모델**"
      ],
      "metadata": {
        "id": "xHua6q5wvdHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, dim, depth, heads,dim_head,attn_dropout,ff_dropout,dimension_i,dimension_o):\n",
        "\n",
        "    super(Model,self).__init__()\n",
        "\n",
        "    self.embedding1= Embedding()\n",
        "    self.transformer1=Transformer( dim, depth, heads,dim_head,attn_dropout,ff_dropout)\n",
        "    self.predict1= nn.Sequential(nn.Flatten(), nn.Linear(dimension_i,dimension_o))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.embedding1(x)\n",
        "    x=self.transformer1(x)\n",
        "    x=self.predict1(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "tCc9q9pGvcpn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "def train(model: Model,\n",
        "          num_epochs: int,\n",
        "          ground_truth: torch.tensor,\n",
        "          input):\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr = 0.001, weight_decay=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        prediction = model(input).reshape(len(input.index))\n",
        "        print(prediction.shape)\n",
        "        loss = loss_fn(prediction, ground_truth)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(epoch+1, \"epoch :\", loss.item())"
      ],
      "metadata": {
        "id": "29Fa17YjvZ7d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_divider(x_train,\n",
        "               y_train,\n",
        "               x_test,\n",
        "               y_test,\n",
        "               batch_size: int):\n",
        "    train_length = int(len(x_train) / batch_size)\n",
        "    test_length = int(len(x_test) / batch_size)\n",
        "\n",
        "    train_dataset = [(x_train.iloc[idx*batch_size:(idx+1)*batch_size, :], torch.FloatTensor(y_train.iloc[idx*batch_size:(idx+1)*batch_size].values).reshape(-1, 1)) for idx in range(train_length)]\n",
        "    train_dataset.append((x_train.iloc[train_length*batch_size:len(x_train), :], torch.FloatTensor(y_train.iloc[train_length*batch_size:len(x_train)].values).reshape(-1, 1)))\n",
        "    test_dataset = [(x_test.iloc[idx*batch_size:(idx+1)*batch_size, :], torch.FloatTensor(y_test.iloc[idx*batch_size:(idx+1)*batch_size].values).reshape(-1, 1)) for idx in range(test_length)]\n",
        "    test_dataset.append((x_test.iloc[test_length*batch_size:len(x_test), :], torch.FloatTensor(y_test.iloc[test_length*batch_size:len(x_test)].values).reshape(-1, 1)))\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def new_train(model: Model,\n",
        "              num_epochs: int,\n",
        "              x_train,\n",
        "              y_train,\n",
        "              x_test,\n",
        "              y_test,\n",
        "              batch_size: int,\n",
        "              opt_lr: float,#learning rate\n",
        "              opt_wd: float,#weight decay\n",
        "              save_model: bool):\n",
        "\n",
        "    test_best_epoch = 0\n",
        "    test_best_loss = 0.0\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr = opt_lr, weight_decay = opt_wd)\n",
        "\n",
        "    train_dataloader, test_dataloader = df_divider(x_train, y_train, x_test, y_test, batch_size)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (x, ground_truth) in enumerate(train_dataloader, 0):\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(x)\n",
        "            loss = loss_fn(prediction, ground_truth)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i%100 == 99:\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        total_test_loss = 0.0\n",
        "        div = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in test_dataloader:\n",
        "                x, ground_truth = data\n",
        "                prediction = model(x)\n",
        "                total_test_loss += loss_fn(prediction, ground_truth)\n",
        "                div += 1\n",
        "\n",
        "        avg_loss = total_test_loss / div\n",
        "        print('MSELoss for test dataset for epoch', epoch+1, ':', avg_loss.item())\n",
        "\n",
        "        if epoch == 0:\n",
        "            test_best_loss = avg_loss\n",
        "        else:\n",
        "            if test_best_loss > avg_loss:\n",
        "                test_best_loss = avg_loss\n",
        "                test_best_epoch = epoch\n",
        "                if save_model:\n",
        "                    path = './best_model.pth'\n",
        "                    torch.save(model.state_dict(), path)\n",
        "\n",
        "    print('The best model was the model from the epoch', test_best_epoch, 'which loss was', test_best_loss)\n",
        "    if save_model:\n",
        "        print('The best model is saved as best_model.pth')"
      ],
      "metadata": {
        "id": "ffFuB9Xgvpfp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDSJdqmKzI1x",
        "outputId": "24f96f6f-33fb-40b2-8a89-b9e3e5aef254"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "vkjFm5fEafEW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.fillna(0)"
      ],
      "metadata": {
        "id": "5tzpBAeqz0mE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx=data[[\"lane\",\"type\",\"speed\",\"capacity\",\"cnt\",\"od\"]]\n",
        "yy=data[\"vol\"]"
      ],
      "metadata": {
        "id": "sH1OiBcezwKh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "vnDUxb3Wz9ku"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(xx,yy,test_size=0.3)"
      ],
      "metadata": {
        "id": "ai2u59stz7dx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "tjhfIWpDzoZZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(5,20,8,64,0.5,0.5,30,1)"
      ],
      "metadata": {
        "id": "Z5N5jhQ6zVk9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train(model, 20, x_train, y_train, x_test, y_test, 16, 0.001, 0.1, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y2S-qCJZ0oa",
        "outputId": "02e1d736-0d49-42f9-bfbb-d36a2c4ce270"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 4067081.091\n",
            "[1,   200] loss: 967972.263\n",
            "[1,   300] loss: 961373.022\n",
            "[1,   400] loss: 879909.167\n",
            "MSELoss for test dataset for epoch 1 : 613906.3125\n",
            "[2,   100] loss: 565947.590\n",
            "[2,   200] loss: 484089.518\n",
            "[2,   300] loss: 522170.768\n",
            "[2,   400] loss: 444000.077\n",
            "MSELoss for test dataset for epoch 2 : 351343.84375\n",
            "[3,   100] loss: 395020.828\n",
            "[3,   200] loss: 385886.597\n",
            "[3,   300] loss: 400576.812\n",
            "[3,   400] loss: 412199.708\n",
            "MSELoss for test dataset for epoch 3 : 342633.6875\n",
            "[4,   100] loss: 379431.896\n",
            "[4,   200] loss: 362921.421\n",
            "[4,   300] loss: 379909.710\n",
            "[4,   400] loss: 397480.459\n",
            "MSELoss for test dataset for epoch 4 : 332956.9375\n",
            "[5,   100] loss: 362770.591\n",
            "[5,   200] loss: 374720.795\n",
            "[5,   300] loss: 392674.833\n",
            "[5,   400] loss: 373706.649\n",
            "MSELoss for test dataset for epoch 5 : 327933.5625\n",
            "[6,   100] loss: 368816.633\n",
            "[6,   200] loss: 374129.838\n",
            "[6,   300] loss: 377237.211\n",
            "[6,   400] loss: 378129.348\n",
            "MSELoss for test dataset for epoch 6 : 332839.4375\n",
            "[7,   100] loss: 355895.992\n",
            "[7,   200] loss: 367298.886\n",
            "[7,   300] loss: 372106.810\n",
            "[7,   400] loss: 374742.890\n",
            "MSELoss for test dataset for epoch 7 : 326371.375\n",
            "[8,   100] loss: 422631.269\n",
            "[8,   200] loss: 377919.426\n",
            "[8,   300] loss: 394684.877\n",
            "[8,   400] loss: 380735.770\n",
            "MSELoss for test dataset for epoch 8 : 330421.09375\n",
            "[9,   100] loss: 376614.475\n",
            "[9,   200] loss: 394605.645\n",
            "[9,   300] loss: 385964.656\n",
            "[9,   400] loss: 390550.846\n",
            "MSELoss for test dataset for epoch 9 : 341279.46875\n",
            "[10,   100] loss: 352122.860\n",
            "[10,   200] loss: 363633.497\n",
            "[10,   300] loss: 389425.258\n",
            "[10,   400] loss: 370390.001\n",
            "MSELoss for test dataset for epoch 10 : 326399.71875\n",
            "[11,   100] loss: 356201.407\n",
            "[11,   200] loss: 369558.018\n",
            "[11,   300] loss: 373265.306\n",
            "[11,   400] loss: 374820.881\n",
            "MSELoss for test dataset for epoch 11 : 322594.5\n",
            "[12,   100] loss: 353549.581\n",
            "[12,   200] loss: 348622.293\n",
            "[12,   300] loss: 381305.687\n",
            "[12,   400] loss: 368101.821\n",
            "MSELoss for test dataset for epoch 12 : 322930.78125\n",
            "[13,   100] loss: 353324.728\n",
            "[13,   200] loss: 353101.782\n",
            "[13,   300] loss: 405232.897\n",
            "[13,   400] loss: 372849.448\n",
            "MSELoss for test dataset for epoch 13 : 339356.8125\n",
            "[14,   100] loss: 350423.768\n",
            "[14,   200] loss: 349851.741\n",
            "[14,   300] loss: 361358.123\n",
            "[14,   400] loss: 382681.123\n",
            "MSELoss for test dataset for epoch 14 : 320877.4375\n",
            "[15,   100] loss: 360418.525\n",
            "[15,   200] loss: 347025.298\n",
            "[15,   300] loss: 361317.115\n",
            "[15,   400] loss: 349959.224\n",
            "MSELoss for test dataset for epoch 15 : 324201.25\n",
            "[16,   100] loss: 360359.684\n",
            "[16,   200] loss: 361901.852\n",
            "[16,   300] loss: 390295.328\n",
            "[16,   400] loss: 397574.558\n",
            "MSELoss for test dataset for epoch 16 : 322213.25\n",
            "[17,   100] loss: 357570.312\n",
            "[17,   200] loss: 363285.197\n",
            "[17,   300] loss: 438321.442\n",
            "[17,   400] loss: 395741.001\n",
            "MSELoss for test dataset for epoch 17 : 328949.625\n",
            "[18,   100] loss: 352907.769\n",
            "[18,   200] loss: 360704.712\n",
            "[18,   300] loss: 357769.060\n",
            "[18,   400] loss: 351517.327\n",
            "MSELoss for test dataset for epoch 18 : 317808.03125\n",
            "[19,   100] loss: 331655.477\n",
            "[19,   200] loss: 355378.681\n",
            "[19,   300] loss: 349652.340\n",
            "[19,   400] loss: 356236.942\n",
            "MSELoss for test dataset for epoch 19 : 319593.0625\n",
            "[20,   100] loss: 330998.060\n",
            "[20,   200] loss: 348778.442\n",
            "[20,   300] loss: 378009.172\n",
            "[20,   400] loss: 363912.027\n",
            "MSELoss for test dataset for epoch 20 : 319272.875\n",
            "The best model was the model from the epoch 17 which loss was tensor(317808.0312)\n"
          ]
        }
      ]
    }
  ]
}